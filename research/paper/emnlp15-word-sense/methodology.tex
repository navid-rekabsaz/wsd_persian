In this section, we introduce our unsupervised approach for Cross Lingual Word Sense Disambiguation. The approach follows the main idea of the Lesk algorithm~\cite{lesk1986automatic}, namely that words in a given context tend to share a common topic. % or, in other words tend to be more similar from semantics point of view. 
 In the absence of external knowledge sources, we use the terms' vector representations to compute their semantic similarity. %In this approach, first using a big corpus, vector representation of the words is created and then using a distance function (i.e. Cosine) the distance between the vectors is computed.   
%Considering the translation of the words in the source language to a group of words in the target language, 
%We apply the mentioned idea to CL-WSD by 
We measure the relatedness of each possible translation of the ambiguous word to all possible translations of the words in its context and select the one that is most similar to the context. %For the translation, we first use a bilingual lexicon %to translate all the words in the source language to all of their possible translations. 
%we use a public online translation API.

To formulate our approach, let us define the list $T$ of translation sets for the words in the context: $T=\{{T_1,T_2,..,T_n} \}$ where $n$ is the number of words in the context, and $T_i$ is the set of translations for the $i^{th}$ word in the context. For each possible translation $t\in T_i$, we also have $P(t)$---an indicator of how frequent this particular translation is.

In general, we compute the similarity of two translations $t$ and $\bar{t}$ as the cosine of their vectors:
\vspace{-0.2cm}
\begin{equation}
  Sim(t,\bar{t}) = \cos(V_t,V_{\bar{t}}) 
  \label{formula:sim2}
\vspace{-0.2cm}
\end{equation}
where $V_t$ is the vector representation of the translation $t$. However, sometimes the translation $t$ of one term in English may be two or more terms in Persian, and thus we will have more than one vector. We thus define a general similarity between two translations:
%Given the sequence of words $S$ in the source language such that $S=\{{s_1,s_2,..,s_n} \}$ where $n$ is the number of words in the sequence, for each word $s_i$ we have a group of translations $T(s_i)=\{ t_{i,1},t_{i,2},..,t_{i,m_i} \}$ such that $m_i$ is the number of possible translations for the word $s_i$. We will denote by $T(S)$ the set of all possible translations for sequence $S$ ($|T(S)|=\prod_{i=1,n}m_i$), and by $T_S\in T(S)$ a possible translation of the sequence. 
%In addition, given the arbitrary translation $t$, $P(t)$ denotes how usual this translation is used regarding to its corresponding word in the source language. 
\vspace{-0.2cm}
\begin{equation}
  Sim(t,\bar{t}) = \max_{w \in t, \bar{w} \in \bar{t}}\left(\cos(V_w,V_{\bar{w}})\right) 
  \label{formula:sim}
\vspace{-0.2cm}
\end{equation}

Obviously when both $t$ and $\bar{t}$ consist of exactly one term, Eq.~\ref{formula:sim} is equivalent to Eq.~\ref{formula:sim2}.
%since each translation may consists of more than one token, we first need to define a way to calculate the similarity between two translations. In order to that, we calculate the similarity between each token pairs using the cosine function and their vector representations and return the highest similarity value:


%where $t$ and $\bar{t}$ denote the translated sequences, $w$ and $\bar{w}$ their tokens, $V_w$ is the vector representation of a word $w$, and $cos$ the cosine function. 

As an example, given the words ``railway'' and ``coach'' and their translations ``\<x.t rAh\hspace{0ex}'Ahn>'' and ``\<wAgn drjh sh>''  with two and respectively three tokens, the Sim function returns cosine between the vectors of ``\<rAh\hspace{0ex}'Ahn>'' and ``\<wAgn>'' (the highest cosine value among the 6 possible combinations). 

In what follows, in order to simplify the annotations, we will use the definition of similarity given by Eq.~\ref{formula:sim2} rather than Eq.~\ref{formula:sim}, thus slightly abusing the notation and using interchangeably $t$ and $V_t$. 


%Given the relatedness between each pair of translations, in the following, we explain our approaches for calculating the semantic relatedness of each candidate translation to its context. 
Having a definition of similarity between two term translations, we now move to defining the similarity between a translation and a set of translations (i.e. the translation of the ambiguous term and the set of possible translations of its context). 
This problem can be approached in two ways: %1. finding the most similar translation of each word in the context and aggregating their vector representations and computing similarity between the achieved vector and the vector of the translation candidate, or 2. identifying the most similar translation between all the other translations to a translation candidate and greedily returning that value~\cite{rekabsaz2015use}.
1. generate one semantic vector for each possible translation of the context (by aggregating the vectors of the term translations that make up this context translation) and compare the translation candidate with it; 2. compare directly the vector of the translation candidate  with all possible term translations vectors in its contexts. In both cases, the candidate translation with the highest score is chosen as the detected sense of the term.

We denote the first approach \emph{RelAgg}. It generates the vector representation of the context using the \emph{contextVec(t,T)} function defined in Algorithm~\ref{alg:contextVec}, %The function creates a representation vector for the context regarding to each candidate translation by aggregating the vectors of the most related translations of each word. the \emph{contextVec(t,T)} function defined in Algorithm~\ref{alg:contextVec},
where %the function $\textnormal{getVec}(t)$ returns the vector representation of the given translation (assuming the translation has only one term), and 
the $\textnormal{normalize}(Vec)$ function normalizes the given vector.

%\vspace{-0.2cm}
\begin{algorithm}[h]
\KwIn{candidate translation $t$, and the list of translation sets $T$}
\KwOut{vector representation of the context}

$sumVec\leftarrow []$\;
\For{$ T_i \in  T$}{
$maxVec\leftarrow []$\;
$maxSim\leftarrow 0$\;
\For{$ \bar{t} \in  T_i$}{
$sim\leftarrow\cos(V_t, V_{\bar{t}} )$\;
    \If{$maxSim<sim$}{
      $maxVec\leftarrow V_{\bar{t}}$\;
      $maxSim\leftarrow sim$\;
    }
}
$sumVec\leftarrow sumVec+maxVec$
}
\textbf{return} $\textnormal{normalize}(sumVec)$\;
\caption{{\bf contextVec Algorithm} \label{alg:contextVec}}
\end{algorithm}
%\vspace{-0.4cm}

Given the vector representation of the context, the \emph{RelAgg} approach is defined as the cosine function between the vectors representation of the candidate translation and the context. The final result is multiplied by the probability of the candidate  translation:
\vspace{-0.2cm}
\begin{multline}
  RelAgg(t,T)=\cos(V_t,contextVec(t,T))\\ \times P(t)
  \label{formula:relagg}
\vspace{-0.4cm}
\end{multline}

The second approach is denoted as \emph{RelGreedy}: 
\vspace{-0.2cm}
\begin{multline}
  \label{formula:relgreedy}
  RelGreedy(t,T)=\max_{T_i \in T}\left(\max_{\bar{t} \in T_i} \left(\cos(t,\bar{t}) \right) \right) \\ \times P(t)
\vspace{-0.4cm}
\end{multline}

In RelGreedy, among all the translations in the context, the value of the most similar one to the candidate translation is returned. Similar to RelAgg, the final score is multiplied by the probability of the candidate translation.

Finally, given the score of the relatedness of each candidate translation to its context using either RelAgg or RelGreedy approach, we can select the best translation among the candidates:
\vspace{-0.2cm}
\begin{equation}
  \label{formula:bestrel}
  Result=\argmax_{t_i}\left(Rel^*(t_i,T)\right)
\vspace{-0.2cm}
\end{equation}
where $t_i$ is a translation candidate for the word with ambiguity, and $Rel^*$ can be replaced by RelAgg or RelGreedy.
