In this section, we describe in detail the process of creating the CL-WSD evaluation benchmark from English to Persian. The created test collection completely matches the output format of the SemEval 2013 CL-WSD task~\cite{lefever2013semeval} and in fact adds a new language to this multilingual task. In addition, we tightly follow the methods in this task for the creation of the gold standard, with only minor alterations necessary in view of the available Persian language resources.    

\vspace{-0.2cm}
\subsection{SemEval 2013 CL-WSD}
The SemEval 2013 Cross-lingual Word Sense Disambiguation task aims to evaluate the viability of multilingual WSD on a benchmark lexical sample data set. Participants should provide contextually correct translations of English ambiguous nouns into five target languages: German, Dutch, French, Italian, and Spanish. The task contains a test set of 20 nouns, each with 50 sentences. 

%The gold standard of the task is based on the Europarl parallel corpus~\cite{europarl}, and on a collection of English sentences entailing lexical sample words, annotated with their appropriate translations.

In order to create the golden standard as described in Lefever et al.~\shortcite{lefeverparallel}, in the first step a sense inventory was constructed based on the possible translations of the ambiguous terms. In order to find the target translations, they ran word alignment on aligned sentences of the Europarl Corpus~\cite{europarl} and manually verified the results. In the next step, the resulting translations were clustered by meaning per focus words. Finally, annotators used this clustered sense inventory to select the correct translation for each word, for up to three translations per word. %If none of the translations was appropriate, they were allowed to chose . 

Two different evaluation methods are used for the task: 1) \emph{Best Result} evaluation, in which the system suggests any number of translations for each target word%that system results to be correct
, and the final score is divided by the number of these translations. 2) \emph{Out-of-five} (OOF) or more relaxed evaluation, in which the system provides up to five different translations, and the final score is that of the best of these five (more details in Lefever et al.~\shortcite{lefever2013semeval}).

\vspace{-0.2cm}
\subsection{New Persian Collection} 
Similar to Lefever et al.~\shortcite{lefeverparallel}, the creation of CL-WSD task for  Persian  consists of two parts: 1) Creating the sense inventory and 2) Annotation of the translations (i.e. the ground truth).

To create the sense inventory for the 20 nouns, due to the lack of a representative parallel corpora, we leveraged three main dictionaries of the Persian language---Aryanpour, Moein, and Farsidic.com---to obtain as large a coverage as possible for their translations. The translations themselves were added by a Persian linguist.

In order to provide a thorough set of translations, in addition to different meanings of nouns,  their idiomatic meanings (in combinations) are also considered. For example, for the word ``pot'', a wide variety of direct translation (\<gldAn>, \<dyg>, \<qwry>, \<ktry>) were selected. However, there is an expression like ``melting pot''%(A place or situation in which different people and cultures get mixed)
, which is not in the dictionaries. These idiomatic meanings were added  to the senses of this word as an expression or equal idiom, which in this case is \<jAm`h\hspace{0ex}y ^cnd n^zAdy>.

The linguist also divided the translations into different senses.% with related translations.
The resulting clusters for nouns range from 2 (e.g., ``education'' to \<t.h.syl> or \<m`rft>) to 6 clusters (e.g., ``post'' to \<pst>, \<mqAm>, \<m.hl mAmwryt>, \<tyr `mwdy>, \<rAy gyry>, \<m`ny A.s.tlA.hy>). The number of translations ranged from 13 for the word ``mood'' to 42 for the word ``ring''\footnote{Available at resources/sense-inventory}. Table~\ref{tbl:statistics} shows details of these statistics.

\begin{table}[t!]
\footnotesize
\caption{Overview of annotators consensus for Persian language}
\vspace{-0.2cm}
\begin{tabular} {c C{0.7cm} C{0.7cm} C{1.3cm} C{1.5cm} }\hline
word  & \# of clus. & \# of trans. & avg. \# clus./sent. & \% clus. consensus \\
\hline\hline
coach&4 &18 & 1.00 & 98 \\\hline
education&2 &15 & 1.02 & 98 \\\hline
execution& 3&14 & 1.08 & 92 \\\hline
figure  &5 & 33& 1.08 & 92 \\\hline
job &3 & 21& 1.02 & 98 \\\hline
letter &4 & 29& 1.04 & 96 \\\hline
match   &3 &19 & 1.04 & 96 \\\hline
mission &3 & 19& 1.02 & 98 \\\hline
mood    & 2& 13& 1.00& 100 \\\hline
paper   & 3& 32& 1.02 & 98 \\\hline
post    & 6& 38& 1.00 & 100 \\\hline
pot     & 4& 34& 1.04 & 96 \\\hline
range   &5 & 36& 1.04 & 96 \\\hline
rest    & 4&40 & 1.00 & 100 \\\hline
ring    & 6& 42& 1.04 & 98 \\\hline
scene    & 4&25 & 1.02 & 98 \\\hline
side     & 3& 32& 1.00 & 96 \\\hline
soil     & 3& 18& 1.00 & 100 \\\hline
strain	& 4& 39& 1.02 & 98 \\\hline
test& 2& 13& 1.00 & 100 \\\hline  
\end{tabular}
\label{tbl:statistics}
\vspace{-0.5cm}
\end{table}

In a second phase, this generated sense inventory is used to annotate the sentences in the test set (50 sentences for each ambiguous word). This phase was performed by three Persian native-speakers. Via a web-based application\footnote{Available at software/webapplication}, annotators choose the appropriate translations in two steps: In the first step, they choose the related sense (cluster). In the second step, the system showed the related translations for the sense, of which they choose up to three translations. In case of no related translation, they choose nothing and continued to the next question. The agreement between annotators is shown in Table~\ref{tbl:statistics} and is similar to that observed by Lefever et al.~\shortcite{lefeverparallel}.

Using the annotated data, we created the gold standard\footnote{Available at resources/golden} in the same format as Lefever et al.~\shortcite{lefever2013semeval}, such that all the evaluation scripts used in the SemEval 2013 CL-WSD task can also be used on this data. Example~\ref{ex:golden1} and Example~\ref{ex:golden2} show the ground truth for two sentences with the word ``coach'':

\begin{exe}
	\ex SENTENCE 2: A branch line train took us to where a \emph{coach} picked us up for the journey up to the camp.\\
      coach.n.fa 2 :: \<Atwbws> 3; \<Atwmbyl> 2; \<wAgn> 1;
\label{ex:golden1}
\end{exe}
\begin{exe}
    \ex SENTENCE 16: Agassi's \emph{coach} came to me with the rackets.\\
        coach.n.fa 16 :: \<mrby> 3; \<mrby wrz^s> 2; \<srmrby> 2; \<m`lm x.sw.sI> 1;
\label{ex:golden2}
\end{exe}

